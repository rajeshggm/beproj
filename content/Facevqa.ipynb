{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mfastText\u001b[0m/  \u001b[01;34mpythia\u001b[0m/  Untitled.ipynb  \u001b[01;34mvqa-maskrcnn-benchmark\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/pythia')\n",
    "sys.path.append('/content/vqa-maskrcnn-benchmark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'maskrcnn_benchmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-cf0ae15418ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmaskrcnn_benchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmaskrcnn_benchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmaskrcnn_benchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_detection_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'maskrcnn_benchmark'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import cv2\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from ipywidgets import widgets, Layout\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "from maskrcnn_benchmark.config import cfg\n",
    "from maskrcnn_benchmark.layers import nms\n",
    "from maskrcnn_benchmark.modeling.detector import build_detection_model\n",
    "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
    "from maskrcnn_benchmark.utils.model_serialization import load_state_dict\n",
    "\n",
    "\n",
    "from pythia.utils.configuration import ConfigNode\n",
    "from pythia.tasks.processors import VocabProcessor, VQAAnswerProcessor\n",
    "from pythia.models.pythia import Pythia\n",
    "from pythia.common.registry import registry\n",
    "from pythia.common.sample import Sample, SampleList\n",
    "\n",
    "\n",
    "class PythiaDemo:\n",
    "  TARGET_IMAGE_SIZE = [448, 448]\n",
    "  CHANNEL_MEAN = [0.485, 0.456, 0.406]\n",
    "  CHANNEL_STD = [0.229, 0.224, 0.225]\n",
    "  \n",
    "  def __init__(self):\n",
    "    self._init_processors()\n",
    "    self.pythia_model = self._build_pythia_model()\n",
    "    self.detection_model = self._build_detection_model()\n",
    "    self.resnet_model = self._build_resnet_model()\n",
    "    \n",
    "  def _init_processors(self):\n",
    "    with open(\"/content/model_data/pythia.yaml\") as f:\n",
    "      config = yaml.load(f)\n",
    "    \n",
    "    config = ConfigNode(config)\n",
    "    # Remove warning\n",
    "    config.training_parameters.evalai_inference = True\n",
    "    registry.register(\"config\", config)\n",
    "    \n",
    "    self.config = config\n",
    "    \n",
    "    vqa_config = config.task_attributes.vqa.dataset_attributes.vqa2\n",
    "    text_processor_config = vqa_config.processors.text_processor\n",
    "    answer_processor_config = vqa_config.processors.answer_processor\n",
    "    \n",
    "    text_processor_config.params.vocab.vocab_file = \"/content/model_data/vocabulary_100k.txt\"\n",
    "    answer_processor_config.params.vocab_file = \"/content/model_data/answers_vqa.txt\"\n",
    "    # Add preprocessor as that will needed when we are getting questions from user\n",
    "    self.text_processor = VocabProcessor(text_processor_config.params)\n",
    "    self.answer_processor = VQAAnswerProcessor(answer_processor_config.params)\n",
    "\n",
    "    registry.register(\"vqa2_text_processor\", self.text_processor)\n",
    "    registry.register(\"vqa2_answer_processor\", self.answer_processor)\n",
    "    registry.register(\"vqa2_num_final_outputs\", \n",
    "                      self.answer_processor.get_vocab_size())\n",
    "    \n",
    "  def _build_pythia_model(self):\n",
    "    state_dict = torch.load('/content/model_data/pythia.pth')\n",
    "    model_config = self.config.model_attributes.pythia\n",
    "    model_config.model_data_dir = \"/content/\"\n",
    "    model = Pythia(model_config)\n",
    "    model.build()\n",
    "    model.init_losses_and_metrics()\n",
    "    \n",
    "    if list(state_dict.keys())[0].startswith('module') and \\\n",
    "       not hasattr(model, 'module'):\n",
    "      state_dict = self._multi_gpu_state_to_single(state_dict)\n",
    "          \n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "  \n",
    "  def _build_resnet_model(self):\n",
    "    self.data_transforms = transforms.Compose([\n",
    "        transforms.Resize(self.TARGET_IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(self.CHANNEL_MEAN, self.CHANNEL_STD),\n",
    "    ])\n",
    "    resnet152 = models.resnet152(pretrained=True)\n",
    "    resnet152.eval()\n",
    "    modules = list(resnet152.children())[:-2]\n",
    "    self.resnet152_model = torch.nn.Sequential(*modules)\n",
    "    self.resnet152_model.to(\"cuda\")\n",
    "  \n",
    "  def _multi_gpu_state_to_single(self, state_dict):\n",
    "    new_sd = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if not k.startswith('module.'):\n",
    "            raise TypeError(\"Not a multiple GPU state of dict\")\n",
    "        k1 = k[7:]\n",
    "        new_sd[k1] = v\n",
    "    return new_sd\n",
    "  \n",
    "  def predict(self, url, question):\n",
    "    with torch.no_grad():\n",
    "      detectron_features = self.get_detectron_features(url)\n",
    "      resnet_features = self.get_resnet_features(url)\n",
    "\n",
    "      sample = Sample()\n",
    "\n",
    "      processed_text = self.text_processor({\"text\": question})\n",
    "      sample.text = processed_text[\"text\"]\n",
    "      sample.text_len = len(processed_text[\"tokens\"])\n",
    "\n",
    "      sample.image_feature_0 = detectron_features\n",
    "      sample.image_info_0 = Sample({\n",
    "          \"max_features\": torch.tensor(100, dtype=torch.long)\n",
    "      })\n",
    "\n",
    "      sample.image_feature_1 = resnet_features\n",
    "\n",
    "      sample_list = SampleList([sample])\n",
    "      sample_list = sample_list.to(\"cuda\")\n",
    "\n",
    "      scores = self.pythia_model(sample_list)[\"scores\"]\n",
    "      scores = torch.nn.functional.softmax(scores, dim=1)\n",
    "      actual, indices = scores.topk(5, dim=1)\n",
    "\n",
    "      top_indices = indices[0]\n",
    "      top_scores = actual[0]\n",
    "\n",
    "      probs = []\n",
    "      answers = []\n",
    "\n",
    "      for idx, score in enumerate(top_scores):\n",
    "        probs.append(score.item())\n",
    "        answers.append(\n",
    "            self.answer_processor.idx2word(top_indices[idx].item())\n",
    "        )\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return probs, answers\n",
    "    \n",
    "  \n",
    "  def _build_detection_model(self):\n",
    "\n",
    "      cfg.merge_from_file('/content/model_data/detectron_model.yaml')\n",
    "      cfg.freeze()\n",
    "\n",
    "      model = build_detection_model(cfg)\n",
    "      checkpoint = torch.load('/content/model_data/detectron_model.pth', \n",
    "                              map_location=torch.device(\"cpu\"))\n",
    "\n",
    "      load_state_dict(model, checkpoint.pop(\"model\"))\n",
    "\n",
    "      model.to(\"cuda\")\n",
    "      model.eval()\n",
    "      return model\n",
    "  \n",
    "  def get_actual_image(self, image_path):\n",
    "      if image_path.startswith('http'):\n",
    "          path = requests.get(image_path, stream=True).raw\n",
    "      else:\n",
    "          path = image_path\n",
    "      \n",
    "      return path\n",
    "\n",
    "  def _image_transform(self, image_path):\n",
    "      path = self.get_actual_image(image_path)\n",
    "\n",
    "      img = Image.open(path)\n",
    "      im = np.array(img).astype(np.float32)\n",
    "      im = im[:, :, ::-1]\n",
    "      im -= np.array([102.9801, 115.9465, 122.7717])\n",
    "      im_shape = im.shape\n",
    "      im_size_min = np.min(im_shape[0:2])\n",
    "      im_size_max = np.max(im_shape[0:2])\n",
    "      im_scale = float(800) / float(im_size_min)\n",
    "      # Prevent the biggest axis from being more than max_size\n",
    "      if np.round(im_scale * im_size_max) > 1333:\n",
    "           im_scale = float(1333) / float(im_size_max)\n",
    "      im = cv2.resize(\n",
    "           im,\n",
    "           None,\n",
    "           None,\n",
    "           fx=im_scale,\n",
    "           fy=im_scale,\n",
    "           interpolation=cv2.INTER_LINEAR\n",
    "       )\n",
    "      img = torch.from_numpy(im).permute(2, 0, 1)\n",
    "      return img, im_scale\n",
    "\n",
    "\n",
    "  def _process_feature_extraction(self, output,\n",
    "                                 im_scales,\n",
    "                                 feat_name='fc6',\n",
    "                                 conf_thresh=0.2):\n",
    "      batch_size = len(output[0][\"proposals\"])\n",
    "      n_boxes_per_image = [len(_) for _ in output[0][\"proposals\"]]\n",
    "      score_list = output[0][\"scores\"].split(n_boxes_per_image)\n",
    "      score_list = [torch.nn.functional.softmax(x, -1) for x in score_list]\n",
    "      feats = output[0][feat_name].split(n_boxes_per_image)\n",
    "      cur_device = score_list[0].device\n",
    "\n",
    "      feat_list = []\n",
    "\n",
    "      for i in range(batch_size):\n",
    "          dets = output[0][\"proposals\"][i].bbox / im_scales[i]\n",
    "          scores = score_list[i]\n",
    "\n",
    "          max_conf = torch.zeros((scores.shape[0])).to(cur_device)\n",
    "\n",
    "          for cls_ind in range(1, scores.shape[1]):\n",
    "              cls_scores = scores[:, cls_ind]\n",
    "              keep = nms(dets, cls_scores, 0.5)\n",
    "              max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep],\n",
    "                                           cls_scores[keep],\n",
    "                                           max_conf[keep])\n",
    "\n",
    "          keep_boxes = torch.argsort(max_conf, descending=True)[:100]\n",
    "          feat_list.append(feats[i][keep_boxes])\n",
    "      return feat_list\n",
    "\n",
    "  def masked_unk_softmax(self, x, dim, mask_idx):\n",
    "      x1 = F.softmax(x, dim=dim)\n",
    "      x1[:, mask_idx] = 0\n",
    "      x1_sum = torch.sum(x1, dim=1, keepdim=True)\n",
    "      y = x1 / x1_sum\n",
    "      return y\n",
    "   \n",
    "  def get_resnet_features(self, image_path):\n",
    "      path = self.get_actual_image(image_path)\n",
    "      img = Image.open(path).convert(\"RGB\")\n",
    "      img_transform = self.data_transforms(img)\n",
    "      \n",
    "      if img_transform.shape[0] == 1:\n",
    "        img_transform = img_transform.expand(3, -1, -1)\n",
    "      img_transform = img_transform.unsqueeze(0).to(\"cuda\")\n",
    "      \n",
    "      features = self.resnet152_model(img_transform).permute(0, 2, 3, 1)\n",
    "      features = features.view(196, 2048)\n",
    "      return features\n",
    "    \n",
    "  def get_detectron_features(self, image_path):\n",
    "      im, im_scale = self._image_transform(image_path)\n",
    "      img_tensor, im_scales = [im], [im_scale]\n",
    "      current_img_list = to_image_list(img_tensor, size_divisible=32)\n",
    "      current_img_list = current_img_list.to('cuda')\n",
    "      with torch.no_grad():\n",
    "          output = self.detection_model(current_img_list)\n",
    "      feat_list = self._process_feature_extraction(output, im_scales, \n",
    "                                                  'fc6', 0.2)\n",
    "      return feat_list[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
